# 슬라이드 13: 언어 모델링 결과: UDLM의 가능성

**(슬라이드 전환 후 잠시 멈춤)**

---

### **슬라이드 소개 및 평가 지표 (0:00 ~ 0:40)**

"자, 이제 우리가 개발한 UDLM 모델과 새로운 가이던스 기법들이 실제 언어 데이터에서 어떤 성능을 보여주는지 살펴보겠습니다. 언어 모델의 성능을 평가하는 가장 대표적인 지표는 **'Perplexity'**입니다.

**Perplexity**는 쉽게 말해, **'이 모델이 다음에 올 단어를 얼마나 잘 예측하는가'**를 나타내는 지표입니다. 마치 문장 빈칸 채우기 게임에서 모델이 정답을 얼마나 잘 맞추는지를 점수화한 것이죠. **Perplexity 값이 낮을수록** 모델이 언어의 패턴을 더 잘 이해하고 자연스러운 문장을 생성할 가능성이 높다고 해석하시면 됩니다."

---

### **주요 결과 테이블 (0:40 ~ 1:20)**

"슬라이드에 보이는 테이블은 다양한 언어 데이터셋에서 **전통적인 AR(Autoregressive) 모델**, **기존 이산 확산 모델인 MDLM**, 그리고 **우리의 새로운 모델 UDLM**의 Perplexity를 비교한 결과입니다.

각 데이터셋의 이름과 함께 해당 데이터셋에서 사용된 단어의 종류(어휘 크기)가 표시되어 있습니다. 테이블의 각 칸에 있는 숫자가 바로 해당 모델의 Perplexity 값입니다."

---

### **핵심 발견: 작은 어휘에서 빛나는 UDLM (1:20 ~ 1:50)**

"결과를 자세히 보시면, 특히 **어휘 크기가 상대적으로 작은 데이터셋**에서 UDLM이 눈에 띄는 성능을 보여줍니다.

예를 들어, 생물 종 이름을 다루는 Species10 (어휘 크기 12)이나 분자 구조를 나타내는 QM9 (어휘 크기 40) 데이터셋에서 **UDLM이 가장 낮은 Perplexity를 기록**했습니다. 특히 QM9 데이터셋에서는 기존 최고 성능인 2.19를 넘어 **2.02라는 새로운 기록을 달성**했습니다. 이는 UDLM의 유연한 구조와 수학적 개선이 특정 종류의 이산 데이터, 특히 단어의 종류가 제한적인 경우에 매우 효과적임을 강력하게 시사합니다."

---

### **다음 단계: 결과의 의미 분석 (1:50 ~ 2:00)**

"물론 어휘 크기가 큰 일반적인 텍스트 데이터셋에서는 아직 AR 모델이나 MDLM이 더 나은 성능을 보이기도 합니다. 이러한 결과들이 우리에게 어떤 중요한 의미를 가지는지, 그리고 UDLM의 잠재력과 앞으로의 연구 방향에 대해 다음 장에서 더 깊이 이야기해 보겠습니다."
