# 슬라이드 13: 언어 모델링 성능 - 전체 결과

**(슬라이드 전환 후 잠시 멈춤)**

---

### **슬라이드 소개 및 평가 지표 설명 (0:00 ~ 0:40)**

"이제 이 연구에서 제안하는 UDLM 모델과 가이던스 기법들이 실제 데이터에서 어떤 성능을 보였는지 구체적인 결과들을 살펴보겠습니다. 먼저 언어 모델링 성능입니다. 언어 모델의 성능은 주로 'Perplexity'라는 지표로 평가합니다. Perplexity는 모델이 다음에 올 단어를 얼마나 잘 예측하는지를 나타내며, '낮을수록' 성능이 좋다고 해석하시면 됩니다."

---

### **결과 테이블 설명 (0:40 ~ 1:20)**

"슬라이드에 보이는 테이블은 다양한 데이터셋에 대해 기존의 AR(Autoregressive) 모델, MDLM, 그리고 이 연구의 UDLM 모델의 Perplexity를 비교한 결과입니다. 각 데이터셋의 특성과 어휘 크기가 함께 표시되어 있습니다.

테이블의 각 행은 다른 데이터셋에서의 성능을 보여주고, 열은 각 모델의 성능 수치를 나타냅니다. AR 모델은 전통적인 순차 생성 모델이고, MDLM은 기존의 마스킹 기반 이산 확산 모델, 그리고 UDLM이 이 연구에서 제안하는 새로운 모델입니다."

---

### **핵심 발견 강조 (1:20 ~ 1:50)**

"결과를 보시면 몇 가지 흥미로운 점을 발견할 수 있습니다. 특히 어휘 크기가 상대적으로 작은 Species10 (어휘 크기 12)이나 QM9 (어휘 크기 40) 데이터셋에서 UDLM이 가장 낮은 Perplexity를 기록하며 강력한 경쟁력을 보여주고 있습니다. QM9 데이터셋에서는 기존 최고 성능인 2.19를 넘어 2.02라는 더 낮은 Perplexity를 달성했습니다.

이는 UDLM의 균등 노이즈 방식과 수학적 개선이 특정 조건, 특히 작은 어휘 크기를 가진 이산 데이터에서 매우 효과적일 수 있음을 시사합니다."

---

### **마무리 및 다음 슬라이드 예고 (1:50 ~ 2:00)**

"물론 어휘 크기가 큰 데이터셋에서는 아직 AR 모델이나 MDLM이 더 나은 성능을 보이기도 합니다. 이러한 결과들이 우리에게 어떤 중요한 인사이트를 주는지, 다음 장에서 더 깊이 분석해 보겠습니다."
